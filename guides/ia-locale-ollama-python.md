# ğŸ§  IA Locale avec Ollama : Guide Complet

> **Niveau** : DÃ©butant  
> **DurÃ©e** : 15 minutes de lecture  
> **MatÃ©riel** : Raspberry Pi (ou tout PC Linux)

---

## ğŸ“‹ Sommaire

1. [L'analogie de la cuisine](#1-lanalogie-de-la-cuisine-comprendre-lenvironnement-virtuel)
2. [Structure technique](#2-structure-technique)
3. [PrÃ©-requis](#3-prÃ©-requis)
4. [Installation pas Ã  pas](#4-installation-pas-Ã -pas)
5. [Configuration de Thonny](#5-configuration-de-thonny)
6. [PremiÃ¨re infÃ©rence](#6-premiÃ¨re-infÃ©rence)
7. [Commandes de rÃ©fÃ©rence](#7-commandes-de-rÃ©fÃ©rence-ollama)
8. [DÃ©pannage](#8-dÃ©pannage)

---

## 1. L'analogie de la cuisine (Comprendre l'Environnement Virtuel)

Imagine que ton Raspberry Pi est une **GRANDE CUISINE partagÃ©e**.

| Ã‰lÃ©ment | Analogie | RÃ´le |
|---------|----------|------|
| **Le SystÃ¨me (OS)** | La cuisine elle-mÃªme (four, Ã©vier, gaz) | Fournit l'infrastructure de base |
| **La Bulle (venv)** | Ton plan de travail portatif | Isole tes projets pour Ã©viter les conflits |
| **Le Moteur (Ollama)** | Le gros frigo de la cuisine | Stocke et exÃ©cute les modÃ¨les d'IA |
| **La TÃ©lÃ©commande (lib `ollama`)** | Tes ustensiles sur le plan de travail | Permet Ã  Python de "parler" au moteur |

### ğŸ¯ Pourquoi cette sÃ©paration ?

```
ProblÃ¨me sans environnement virtuel :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Projet A utilise numpy 1.20        â”‚
â”‚  Projet B utilise numpy 2.0         â”‚
â”‚  â†’ CONFLIT : Impossible d'avoir     â”‚
â”‚    les deux versions en mÃªme temps !â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Solution avec environnement virtuel :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [venv-projetA]  numpy 1.20  âœ“      â”‚
â”‚  [venv-projetB]  numpy 2.0   âœ“      â”‚
â”‚  â†’ Chaque projet a ses propres      â”‚
â”‚    bibliothÃ¨ques, pas de conflit    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Structure technique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RASPBERRY PI (OS)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  MOTEUR : Ollama (installÃ© sur le systÃ¨me)          â”‚   â”‚
â”‚  â”‚  ğŸ“ /usr/local/bin/ollama                           â”‚   â”‚
â”‚  â”‚  ğŸ“ ~/.ollama/models/  â† Les modÃ¨les sont ici       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â–²                                â”‚
â”‚                            â”‚ API locale (HTTP)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ENVIRONNEMENT VIRTUEL : mon_ia_env                  â”‚  â”‚
â”‚  â”‚  ğŸ“ /home/pi/mon_ia_env/                             â”‚  â”‚
â”‚  â”‚                                                      â”‚  â”‚
â”‚  â”‚  BibliothÃ¨ques installÃ©es :                          â”‚  â”‚
â”‚  â”‚    â””â”€â”€ ollama (la "tÃ©lÃ©commande" Python)             â”‚  â”‚
â”‚  â”‚                                                      â”‚  â”‚
â”‚  â”‚  Ton code Python :                                   â”‚  â”‚
â”‚  â”‚    from ollama import chat                           â”‚  â”‚
â”‚  â”‚    response = chat(model='llama3.2', ...)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. PrÃ©-requis

- [ ] Raspberry Pi avec Raspberry Pi OS (ou tout Linux)
- [ ] Connexion internet
- [ ] Terminal ouvert
- [ ] Python 3 installÃ© (vÃ©rifier avec `python3 --version`)

---

## 4. Installation pas Ã  pas

### Ã‰tape 1 : Installer Ollama (le moteur)

```bash
# Commande officielle d'installation
curl -fsSL https://ollama.com/install.sh | sh
```

> âš ï¸ **Attention** : L'installation peut prendre plusieurs minutes sur Raspberry Pi.

VÃ©rifier l'installation :
```bash
ollama --version
```

### Ã‰tape 2 : CrÃ©er l'environnement virtuel

```bash
# Se placer dans le dossier utilisateur
cd ~

# CrÃ©er la "bulle" (le plan de travail)
python3 -m venv mon_ia_env

# RÃ©sultat : un nouveau dossier 'mon_ia_env' est crÃ©Ã©
ls -la mon_ia_env/
```

### Ã‰tape 3 : Activer l'environnement

```bash
# Activer la bulle (mettre ses gants de cuisine)
source mon_ia_env/bin/activate

# Tu dois voir (mon_ia_env) au dÃ©but de ta ligne de commande :
# (mon_ia_env) pi@raspberrypi:~ $
```

### Ã‰tape 4 : Installer la bibliothÃ¨que Python

```bash
# Dans l'environnement activÃ© (tu vois le prÃ©fixe)
pip install ollama

# VÃ©rifier l'installation
pip list | grep ollama
```

### Ã‰tape 5 : TÃ©lÃ©charger un modÃ¨le

```bash
# TÃ©lÃ©charger Llama 3.2 (version lÃ©gÃ¨re, parfait pour Raspberry Pi)
ollama pull llama3.2

# Voir les modÃ¨les disponibles
ollama list
```

---

## 5. Configuration de Thonny

Pour que Thonny utilise ton environnement virtuel :

1. **Ouvrir Thonny** (menu â†’ Programmation â†’ Thonny)
2. **Menu** : `Outils` â†’ `Options`
3. **Onglet** : `InterprÃ©teur`
4. **Choisir** : `Alternative Python 3 interpreter or virtual environment`
5. **Cliquer** sur `...` Ã  droite de l'exÃ©cutable Python
6. **Naviguer vers** : `/home/pi/mon_ia_env/bin/python`
7. **RedÃ©marrer Thonny** pour appliquer les changements

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Options â†’ InterprÃ©teur                      â”‚
â”‚                                              â”‚
â”‚  â—‹ The same interpreter that runs Thonny     â”‚
â”‚  â— Alternative Python 3 interpreter...       â”‚
â”‚    ExÃ©cutable Python :                       â”‚
â”‚    [/home/pi/mon_ia_env/bin/python] [...]    â”‚
â”‚                                              â”‚
â”‚              [ OK ]    [ Annuler ]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. PremiÃ¨re infÃ©rence

CrÃ©e un nouveau fichier dans Thonny (`Fichier` â†’ `Nouveau`) et colle ce code :

```python
"""
Premier chat avec un modÃ¨le local Ollama
"""
from ollama import chat

# Envoyer une question au modÃ¨le
response = chat(
    model='llama3.2',
    messages=[
        {'role': 'user', 'content': 'Explique-moi ce qu\'est un Raspberry Pi en 3 phrases'}
    ]
)

# Afficher la rÃ©ponse
print("ğŸ¤– RÃ©ponse du modÃ¨le :")
print(response.message.content)
```

**RÃ©sultat attendu :**
```
ğŸ¤– RÃ©ponse du modÃ¨le :
Le Raspberry Pi est un ordinateur monocarte de la taille d'une carte de crÃ©dit, 
conÃ§u pour promouvoir l'enseignement de l'informatique et l'expÃ©rimentation 
avec le matÃ©riel. Il peut exÃ©cuter un systÃ¨me d'exploitation complet comme 
Linux et servir de base pour des projets allant de serveurs domestiques Ã  
des robots. Son faible coÃ»t et sa communautÃ© active en font un outil idÃ©al 
pour les hobbyistes et les Ã©ducateurs.
```

### ğŸ“ Exemple avec historique de conversation

```python
from ollama import chat

# Conversation avec mÃ©moire du contexte
messages = []

# Premier message
messages.append({'role': 'user', 'content': 'Bonjour, je m\'appelle Madi'})
response = chat(model='llama3.2', messages=messages)
messages.append({'role': 'assistant', 'content': response.message.content})
print(f"Assistant: {response.message.content}\n")

# Le modÃ¨le se souvient de ton prÃ©nom
messages.append({'role': 'user', 'content': 'Quel est mon prÃ©nom ?'})
response = chat(model='llama3.2', messages=messages)
print(f"Assistant: {response.message.content}")
```

---

## 7. Commandes de rÃ©fÃ©rence Ollama

### Gestion des modÃ¨les

```bash
# Lister les modÃ¨les installÃ©s
ollama list

# TÃ©lÃ©charger un nouveau modÃ¨le
ollama pull llama3.2
ollama pull gemma2:2b      # Version ultra-lÃ©gÃ¨re
ollama pull phi3:mini      # Petit modÃ¨le performant

# Supprimer un modÃ¨le
ollama rm llama3.2

# Lancer le modÃ¨le en mode interactif (chat en ligne de commande)
ollama run llama3.2

# Quitter le mode interactif : Ctrl+D ou taper /bye
```

### Gestion du service

```bash
# VÃ©rifier si Ollama tourne
systemctl status ollama

# DÃ©marrer Ollama
sudo systemctl start ollama

# ArrÃªter Ollama
sudo systemctl stop ollama

# RedÃ©marrer Ollama
sudo systemctl restart ollama

# Activer le dÃ©marrage automatique
sudo systemctl enable ollama
```

### Environnement virtuel Python

```bash
# CrÃ©er un venv
python3 -m venv nom_env

# Activer (Linux/Mac)
source nom_env/bin/activate

# Activer (Windows)
nom_env\Scripts\activate

# DÃ©sactiver
deactivate

# Voir oÃ¹ on est
which python
pip list
```

---

## 8. DÃ©pannage

### âŒ ProblÃ¨me : "ollama: command not found"

**Cause** : Ollama n'est pas dans le PATH

**Solution** :
```bash
# Ajouter au PATH
export PATH=$PATH:/usr/local/bin

# Ou relancer le terminal aprÃ¨s installation
```

### âŒ ProblÃ¨me : "ModuleNotFoundError: No module named 'ollama'"

**Cause** : La bibliothÃ¨que n'est pas installÃ©e dans l'environnement actif

**Solution** :
```bash
# VÃ©rifier que l'environnement est actif (tu dois voir le nom entre parenthÃ¨ses)
source ~/mon_ia_env/bin/activate

# RÃ©installer
pip install ollama
```

### âŒ ProblÃ¨me : "Error: could not connect to ollama server"

**Cause** : Le service Ollama ne tourne pas

**Solution** :
```bash
# DÃ©marrer le service
sudo systemctl start ollama

# VÃ©rifier le statut
sudo systemctl status ollama
```

### âŒ ProblÃ¨me : RÃ©ponses trÃ¨s lentes

**Cause** : Raspberry Pi a des ressources limitÃ©es

**Solutions** :
- Utiliser des modÃ¨les plus petits : `llama3.2`, `gemma2:2b`, `phi3:mini`
- Augmenter le swap si nÃ©cessaire
- Fermer les autres applications

### âŒ ProblÃ¨me : Thonny n'utilise pas le bon Python

**VÃ©rification** :
```python
import sys
print(sys.executable)
# Doit afficher : /home/pi/mon_ia_env/bin/python
```

**Solution** : Reconfigurer l'interprÃ©teur dans les options de Thonny (voir section 5)

---

## ğŸ“š Ressources complÃ©mentaires

- **Documentation Ollama** : https://github.com/ollama/ollama
- **ModÃ¨les disponibles** : https://ollama.com/library
- **BibliothÃ¨que Python** : `pip show ollama`

---

*Guide crÃ©Ã© pour l'apprentissage de l'IA locale sur Raspberry Pi.*  
*DerniÃ¨re mise Ã  jour : FÃ©vrier 2026*
