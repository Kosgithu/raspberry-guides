# Anatomie d'un Code d'Inference (Hugging Face)

## Qu'est-ce que l'Inference ?

L'**inference**, c'est l'action de demander Ã  une IA dÃ©jÃ  entraÃ®nÃ©e de **"dÃ©duire"** une rÃ©ponse Ã  partir d'une entrÃ©e (ton prompt). 

> ğŸ’¡ **Analogie simple** : C'est comme demander Ã  un expert son avis. L'expert (le modÃ¨le) a dÃ©jÃ  fait ses Ã©tudes (l'entraÃ®nement), tu lui poses une question (l'input) et il te donne sa rÃ©ponse (l'output).

Le code pour faire Ã§a via API est une **recette standardisÃ©e** qui suit toujours la mÃªme structure.

---

## ğŸ—ºï¸ Les 5 Ã‰lÃ©ments Indispensables

| Ã‰tape | Nom technique | RÃ´le | MÃ©taphore |
|-------|---------------|------|-----------|
| **1** | `API_URL` | L'adresse exacte du modÃ¨le sur les serveurs | ğŸ“ **L'adresse** du cerveau spÃ©cifique |
| **2** | `Headers` / Token | Ton identifiant pour accÃ©der au service | ğŸªª **La carte d'identitÃ©** |
| **3** | `Payload` / Inputs | Les donnÃ©es que tu envoies (texte, image, son) | â“ **La question** posÃ©e |
| **4** | `Request` (POST) | L'action d'envoyer et d'attendre | â³ **L'attente** du retour |
| **5** | `Output` / Response | Ce que l'IA gÃ©nÃ¨re | ğŸ’¬ **La rÃ©ponse** |

---

## ğŸ”¬ Le Code Complet CommentÃ©

```python
import requests  # La bibliothÃ¨que qui permet Ã  ton Pi de "parler" Ã  Internet

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  1. L'ADRESSE (Le modÃ¨le prÃ©cis)                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Structure standard : site + service + auteur/nom_du_modele
API_URL = "https://api-inference.huggingface.co/models/tencent/HunyuanImage-3.0-Instruct"

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  2. LA CARTE D'IDENTITÃ‰ (Le Token)                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Le mot "Bearer" est obligatoire avant le token (norme OAuth 2.0)
headers = {
    "Authorization": f"Bearer hf_VOTRE_TOKEN_ICI"
}

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  3. LA FONCTION D'ENVOI (La machine Ã  communiquer)               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def interroger_ia(donnees):
    """
    Envoie une requÃªte POST au serveur Hugging Face
    POST = "je dÃ©pose des donnÃ©es sur le serveur"
    """
    try:
        reponse = requests.post(API_URL, headers=headers, json=donnees)
        reponse.raise_for_status()  # VÃ©rifie si erreur HTTP (404, 401, etc.)
        return reponse.json()       # Convertit la rÃ©ponse en dictionnaire Python
    except requests.exceptions.RequestException as e:
        print(f"âŒ Erreur de connexion : {e}")
        return None

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  4. LE PAYLOAD (Ton instruction dÃ©taillÃ©e)                       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
mon_ordre = {
    "inputs": "Un boÃ®tier de Raspberry Pi 5 imprimÃ© en 3D, style industriel.",
    "parameters": {  # Optionnel : rÃ©glages avancÃ©s
        "width": 512,
        "height": 512
    }
}

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  5. L'EXÃ‰CUTION                                                  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == "__main__":
    print("ğŸš€ Envoi de la requÃªte...")
    resultat = interroger_ia(mon_ordre)
    
    if resultat:
        print("âœ… RÃ©ponse reÃ§ue :")
        print(resultat)
    else:
        print("âŒ Ã‰chec de la requÃªte")
```

---

## ğŸ¯ Ce qui Change Selon le Type de ModÃ¨le

La **structure reste identique**, mais le contenu du `payload` varie :

### ğŸ–¼ï¸ ModÃ¨le d'Image (GÃ©nÃ©ration)
```python
payload = {
    "inputs": "Un chat astronaute sur la lune",  # Ta description
    "parameters": {
        "width": 512,
        "height": 512,
        "guidance_scale": 7.5  # Respect du prompt (1-20)
    }
}
# Retour : Tableau d'octets (bytes) â†’ Ã  sauvegarder en .png/.jpg
```

### ğŸ’¬ ModÃ¨le de Texte (Chat/LLM)
```python
payload = {
    "inputs": "Explique la computer vision en 3 phrases simples",
    "parameters": {
        "max_new_tokens": 100,  # Longueur max de la rÃ©ponse
        "temperature": 0.7      # CrÃ©ativitÃ© (0=prÃ©cis, 1=crÃ©atif)
    }
}
# Retour : Dictionnaire avec clÃ© "generated_text"
```

### ğŸ”§ ModÃ¨le d'Analyse (Classification)
```python
payload = {
    "inputs": "Ce produit est excellent !",  # Texte Ã  analyser
    "options": {
        "wait_for_model": True  # RÃ©veille le serveur si inactif
    }
}
# Retour : Liste de labels avec scores de confiance
```

---

## ğŸš¨ Troubleshooting (Erreurs Courantes)

| Erreur | Cause probable | Solution |
|--------|---------------|----------|
| `401 Unauthorized` | Token invalide ou manquant | VÃ©rifie ton Bearer token sur Hugging Face |
| `404 Not Found` | URL du modÃ¨le incorrecte | VÃ©rifie le nom exact sur la page du modÃ¨le |
| `503 Service Unavailable` | ModÃ¨le en chargement | Ajoute `"wait_for_model": True` dans options |
| `Timeout` | RÃ©ponse trop longue | Augmente `timeout=60` dans requests.post() |
| `JSONDecodeError` | RÃ©ponse vide ou non-JSON | VÃ©rifie si le modÃ¨le retourne du binaire (image) |

### ğŸ” Checklist de DÃ©bogage (dans l'ordre)
1. âœ… L'URL est-elle complÃ¨te et correcte ?
2. âœ… Le token est-il valide et prÃ©cÃ©dÃ© de "Bearer " ?
3. âœ… Le payload est-il au format JSON valide ?
4. âœ… As-tu une connexion Internet active ?
5. âœ… Le modÃ¨le est-il en ligne sur Hugging Face ?

> ğŸ’¡ **RÃ¨gle des 90%** : 90% des erreurs viennent d'une URL mal Ã©crite ou d'un Token oubliÃ©.

---

## ğŸ§  Ã€ Retenir (La Structure en 5 Mots)

```
IMPORT â†’ URL â†’ HEADERS â†’ PAYLOAD â†’ RESPONSE
```

Ou en mnÃ©monique : **"Ils Utilisent des Headers Pour Recevoir"**

**Ne mÃ©morise pas le code caractÃ¨re par caractÃ¨re.** MÃ©morise cette structure. Si un jour ton code ne marche pas, vÃ©rifie toujours dans cet ordre.

---

## ğŸ› ï¸ Le Script Universel (Passe-Partout)

Ce script avancÃ© dÃ©tecte **automatiquement** le type de rÃ©ponse (image ou texte) et s'adapte Ã  n'importe quel modÃ¨le Hugging Face.

```python
import requests
import io
from PIL import Image  # Pour afficher/sauvegarder les images

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CONFIGURATION (Les 2 seules lignes Ã  changer)                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TOKEN = "hf_VOTRE_TOKEN_ICI"
MODEL_URL = "https://api-inference.huggingface.co/models/tencent/HunyuanImage-3.0-Instruct"
# Exemples d'URL alternatifs :
# MODEL_URL = "https://api-inference.huggingface.co/models/gpt2"  # Texte
# MODEL_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"  # RÃ©sumÃ©

def interroger_ia(prompt):
    """
    Fonction universelle : dÃ©tecte automatiquement si la rÃ©ponse
    est une image (bytes) ou du texte (JSON).
    """
    headers = {"Authorization": f"Bearer {TOKEN}"}
    payload = {"inputs": prompt}
    
    print(f"ğŸš€ Envoi de la requÃªte au modÃ¨le...")
    
    try:
        response = requests.post(MODEL_URL, headers=headers, json=payload, timeout=120)
        response.raise_for_status()
        
        # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        # â•‘  DÃ‰TECTION AUTOMATIQUE DU TYPE DE RÃ‰PONSE                   â•‘
        # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        content_type = response.headers.get('content-type', '')
        
        if "image" in content_type:
            # ğŸ–¼ï¸ RÃ‰PONSE IMAGE
            print("ğŸ“¸ Image reÃ§ue ! Sauvegarde en cours...")
            image = Image.open(io.BytesIO(response.content))
            filename = "resultat_ia.png"
            image.save(filename)
            return f"âœ… Image sauvegardÃ©e sous : {filename}"
            
        elif "json" in content_type:
            # ğŸ’¬ RÃ‰PONSE TEXTE (JSON)
            print("âœï¸ Texte reÃ§u !")
            data = response.json()
            
            # Extraction intelligente du texte selon le format
            if isinstance(data, list) and len(data) > 0:
                if "generated_text" in data[0]:
                    return data[0]["generated_text"]
                elif "summary_text" in data[0]:
                    return data[0]["summary_text"]
                else:
                    return data[0]
            elif isinstance(data, dict):
                return data.get("generated_text", str(data))
            else:
                return str(data)
        else:
            # â“ TYPE INCONNU
            return f"âš ï¸ Type de rÃ©ponse inattendu : {content_type}"
            
    except requests.exceptions.Timeout:
        return "â±ï¸ Timeout : Le modÃ¨le met trop de temps Ã  rÃ©pondre (essayez 'wait_for_model': True)"
    except requests.exceptions.RequestException as e:
        return f"âŒ Erreur de connexion : {e}"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXEMPLES D'UTILISATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # ğŸ–¼ï¸ Test avec un modÃ¨le d'image
    # mon_prompt = "Un logo futuriste pour un projet de Raspberry Pi 5"
    
    # ğŸ’¬ Test avec un modÃ¨le de texte
    mon_prompt = "Explique la computer vision en 3 phrases"
    
    resultat = interroger_ia(mon_prompt)
    print("\n" + "="*50)
    print("RÃ‰SULTAT :")
    print("="*50)
    print(resultat)
```

### ğŸ¯ Pourquoi c'est un "Passe-Partout" ?

| FonctionnalitÃ© | Avantage |
|----------------|----------|
| **DÃ©tection automatique** | Fonctionne avec image ET texte sans modifier le code |
| **Gestion du timeout** | Ne plante pas si le modÃ¨le "dort" (120s d'attente) |
| **Extraction intelligente** | Adapte l'affichage selon le format de rÃ©ponse du modÃ¨le |
| **Sauvegarde auto** | Les images sont enregistrÃ©es directement sur le SSD |

### ğŸ”§ Comment l'adapter Ã  ton projet ?

1. **Remplace** `TOKEN` par ton vrai token Hugging Face
2. **Remplace** `MODEL_URL` par l'URL du modÃ¨le voulu
3. **Modifie** `mon_prompt` avec ta requÃªte
4. **Lance** le script â†’ Il fait le reste automatiquement !

### ğŸ“‹ URLs de modÃ¨les populaires Ã  tester

| Type | ModÃ¨le | URL Ã  utiliser |
|------|--------|----------------|
| ğŸ–¼ï¸ Image | HunyuanImage | `models/tencent/HunyuanImage-3.0-Instruct` |
| ğŸ–¼ï¸ Image | Stable Diffusion | `models/stabilityai/stable-diffusion-xl-base-1.0` |
| ğŸ’¬ Texte | GPT-2 | `models/gpt2` |
| ğŸ’¬ Texte | BART (rÃ©sumÃ©) | `models/facebook/bart-large-cnn` |
| ğŸ”§ Analyse | Sentiment | `models/distilbert-base-uncased-finetuned-sst-2-english` |

---

## ğŸ“š Ressources Utiles

- **Documentation Hugging Face Inference API** : https://huggingface.co/docs/api-inference
- **Trouver des modÃ¨les** : https://huggingface.co/models
- **GÃ©nÃ©rer un token** : https://huggingface.co/settings/tokens

---

*DerniÃ¨re mise Ã  jour : 2026-02-24*
